{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5522a7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4880, 55)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('stratified_upsample.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd4323e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>hipFracture</th>\n",
       "      <th>spineFracture</th>\n",
       "      <th>otherAdultFracture</th>\n",
       "      <th>fractureAge</th>\n",
       "      <th>fracturedBones</th>\n",
       "      <th>priorDexaScan</th>\n",
       "      <th>lastDexaScanDetails</th>\n",
       "      <th>fragilityFractureAfter45</th>\n",
       "      <th>youngLowTraumaFracture</th>\n",
       "      <th>...</th>\n",
       "      <th>recentWeightLoss</th>\n",
       "      <th>recentHeightLoss</th>\n",
       "      <th>spineSurgery</th>\n",
       "      <th>hipSurgery</th>\n",
       "      <th>gastricSurgery</th>\n",
       "      <th>currentlyPregnant</th>\n",
       "      <th>llm_data</th>\n",
       "      <th>llm_prediction</th>\n",
       "      <th>input_weight</th>\n",
       "      <th>total_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>34.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>42.0</td>\n",
       "      <td>Hip (Proximal Femur), Spine (Vertebrae), Pelvis</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>34.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>34.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>77.0</td>\n",
       "      <td>Wrist (Distal Radius), Humerus (Upper Arm Bone)</td>\n",
       "      <td>No</td>\n",
       "      <td>2024 at General hospital</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>10.5</td>\n",
       "      <td>34.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Spine (Vertebrae), Humerus (Upper Arm Bone)</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>34.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender hipFracture spineFracture otherAdultFracture  fractureAge  \\\n",
       "0  Female          No            No                 No          NaN   \n",
       "1  Female          No            No                Yes         42.0   \n",
       "2  Female          No            No                 No          NaN   \n",
       "3  Female          No            No                 No         77.0   \n",
       "4  Female          No           Yes                 No         66.0   \n",
       "\n",
       "                                    fracturedBones priorDexaScan  \\\n",
       "0                                              NaN            No   \n",
       "1  Hip (Proximal Femur), Spine (Vertebrae), Pelvis            No   \n",
       "2                                              NaN            No   \n",
       "3  Wrist (Distal Radius), Humerus (Upper Arm Bone)            No   \n",
       "4      Spine (Vertebrae), Humerus (Upper Arm Bone)            No   \n",
       "\n",
       "         lastDexaScanDetails fragilityFractureAfter45 youngLowTraumaFracture  \\\n",
       "0                        NaN                       No                     No   \n",
       "1                        NaN                       No                    Yes   \n",
       "2                        NaN                       No                     No   \n",
       "3  2024 at General hospital                        No                    Yes   \n",
       "4                        NaN                      Yes                     No   \n",
       "\n",
       "   ... recentWeightLoss recentHeightLoss spineSurgery hipSurgery  \\\n",
       "0  ...               No               No           No         No   \n",
       "1  ...               No               No           No         No   \n",
       "2  ...               No              Yes           No         No   \n",
       "3  ...              Yes              Yes          Yes         No   \n",
       "4  ...               No               No           No         No   \n",
       "\n",
       "   gastricSurgery currentlyPregnant  \\\n",
       "0              No                No   \n",
       "1              No                No   \n",
       "2              No                No   \n",
       "3             Yes                No   \n",
       "4              No               Yes   \n",
       "\n",
       "                                            llm_data  \\\n",
       "0  \\n    The patient has reported a history of hi...   \n",
       "1  \\n    The patient has reported a history of hi...   \n",
       "2  \\n    The patient has reported a history of hi...   \n",
       "3  \\n    The patient has reported a history of hi...   \n",
       "4  \\n    The patient has reported a history of hi...   \n",
       "\n",
       "                                      llm_prediction input_weight total_weight  \n",
       "0  \\n    The patient has reported a history of hi...          4.0         34.5  \n",
       "1  \\n    The patient has reported a history of hi...          8.0         34.5  \n",
       "2  \\n    The patient has reported a history of hi...          5.0         34.5  \n",
       "3  \\n    The patient has reported a history of hi...         10.5         34.5  \n",
       "4  \\n    The patient has reported a history of hi...         12.0         34.5  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "837c7960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>hipFracture</th>\n",
       "      <th>spineFracture</th>\n",
       "      <th>otherAdultFracture</th>\n",
       "      <th>fractureAge</th>\n",
       "      <th>fracturedBones</th>\n",
       "      <th>priorDexaScan</th>\n",
       "      <th>lastDexaScanDetails</th>\n",
       "      <th>fragilityFractureAfter45</th>\n",
       "      <th>youngLowTraumaFracture</th>\n",
       "      <th>...</th>\n",
       "      <th>recentHeightLoss</th>\n",
       "      <th>spineSurgery</th>\n",
       "      <th>hipSurgery</th>\n",
       "      <th>gastricSurgery</th>\n",
       "      <th>currentlyPregnant</th>\n",
       "      <th>llm_data</th>\n",
       "      <th>llm_prediction</th>\n",
       "      <th>input_weight</th>\n",
       "      <th>total_weight</th>\n",
       "      <th>risk_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>34.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>42.0</td>\n",
       "      <td>Hip (Proximal Femur), Spine (Vertebrae), Pelvis</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>34.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>34.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>77.0</td>\n",
       "      <td>Wrist (Distal Radius), Humerus (Upper Arm Bone)</td>\n",
       "      <td>No</td>\n",
       "      <td>2024 at General hospital</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>10.5</td>\n",
       "      <td>34.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Spine (Vertebrae), Humerus (Upper Arm Bone)</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>34.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender hipFracture spineFracture otherAdultFracture  fractureAge  \\\n",
       "0  Female          No            No                 No          NaN   \n",
       "1  Female          No            No                Yes         42.0   \n",
       "2  Female          No            No                 No          NaN   \n",
       "3  Female          No            No                 No         77.0   \n",
       "4  Female          No           Yes                 No         66.0   \n",
       "\n",
       "                                    fracturedBones priorDexaScan  \\\n",
       "0                                              NaN            No   \n",
       "1  Hip (Proximal Femur), Spine (Vertebrae), Pelvis            No   \n",
       "2                                              NaN            No   \n",
       "3  Wrist (Distal Radius), Humerus (Upper Arm Bone)            No   \n",
       "4      Spine (Vertebrae), Humerus (Upper Arm Bone)            No   \n",
       "\n",
       "         lastDexaScanDetails fragilityFractureAfter45 youngLowTraumaFracture  \\\n",
       "0                        NaN                       No                     No   \n",
       "1                        NaN                       No                    Yes   \n",
       "2                        NaN                       No                     No   \n",
       "3  2024 at General hospital                        No                    Yes   \n",
       "4                        NaN                      Yes                     No   \n",
       "\n",
       "   ... recentHeightLoss spineSurgery hipSurgery gastricSurgery  \\\n",
       "0  ...               No           No         No             No   \n",
       "1  ...               No           No         No             No   \n",
       "2  ...              Yes           No         No             No   \n",
       "3  ...              Yes          Yes         No            Yes   \n",
       "4  ...               No           No         No             No   \n",
       "\n",
       "   currentlyPregnant                                           llm_data  \\\n",
       "0                 No  \\n    The patient has reported a history of hi...   \n",
       "1                 No  \\n    The patient has reported a history of hi...   \n",
       "2                 No  \\n    The patient has reported a history of hi...   \n",
       "3                 No  \\n    The patient has reported a history of hi...   \n",
       "4                Yes  \\n    The patient has reported a history of hi...   \n",
       "\n",
       "                                      llm_prediction input_weight  \\\n",
       "0  \\n    The patient has reported a history of hi...          4.0   \n",
       "1  \\n    The patient has reported a history of hi...          8.0   \n",
       "2  \\n    The patient has reported a history of hi...          5.0   \n",
       "3  \\n    The patient has reported a history of hi...         10.5   \n",
       "4  \\n    The patient has reported a history of hi...         12.0   \n",
       "\n",
       "  total_weight risk_level  \n",
       "0         34.5          2  \n",
       "1         34.5          1  \n",
       "2         34.5          2  \n",
       "3         34.5          2  \n",
       "4         34.5          2  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_risk_level(text):\n",
    "  res = re.findall(r'(\\d+)% risk', text)\n",
    "  risk = int(res[0])\n",
    "\n",
    "  if risk < 10:\n",
    "    return 0\n",
    "  elif risk < 20:\n",
    "    return 1\n",
    "  return 2\n",
    "\n",
    "df['risk_level'] = df['llm_prediction'].apply(get_risk_level)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "41cf1365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low: (1135, 58)\n",
      "Medium: (207, 58)\n",
      "High: (3538, 58)\n"
     ]
    }
   ],
   "source": [
    "print('Low:', df[df['risk_level'] == 0].shape)\n",
    "print('Medium:', df[df['risk_level'] == 1].shape)\n",
    "print('High:', df[df['risk_level'] == 2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "43b75729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip3 install torch torchvision torchaudio\n",
    "# !pip install tf-keras==2.16.0 --no-dependencies\n",
    "\n",
    "\n",
    "# !conda install -n .conda ipykernel --update-deps --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7323ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cda6f20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "roberta_pipe = pipeline(\n",
    "    \"text-classification\", model=\"aaslan47/robota-sequence-classifier\"\n",
    ")\n",
    "\n",
    "distilbert_pipe = pipeline(\n",
    "    \"text-classification\", model=\"aaslan47/distilbert-sequence-classifier\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_result(res):\n",
    "  label = res[0]['label']\n",
    "  if label == 'LABEL_0':\n",
    "    return 0\n",
    "  elif label == 'LABEL_1':\n",
    "    return 1\n",
    "  else:\n",
    "    print(label)\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f26ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a5363a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9dd6bc67cd41478f594a6065327c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllm_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: parse_result(roberta_pipe([x])))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tqdm/std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(df, df_function)(wrapper, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4919\u001b[0m         func,\n\u001b[1;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1509\u001b[0m )\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tqdm/std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllm_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: parse_result(roberta_pipe([x])))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:163\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,)\n\u001b[0;32m--> 163\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:1445\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1442\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1443\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1444\u001b[0m     )\n\u001b[0;32m-> 1445\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(final_iterator)\n\u001b[1;32m   1446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:1371\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1370\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1371\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1372\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:194\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    193\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:1191\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;124;03mtoken_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;124;03m    Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1191\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroberta(\n\u001b[1;32m   1192\u001b[0m     input_ids,\n\u001b[1;32m   1193\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1194\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1195\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1196\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1197\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1198\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1199\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1200\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1201\u001b[0m )\n\u001b[1;32m   1202\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1203\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:858\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    856\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 858\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    859\u001b[0m     embedding_output,\n\u001b[1;32m    860\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m    861\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    862\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m    863\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m    864\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    865\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    866\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    867\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    868\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    869\u001b[0m )\n\u001b[1;32m    870\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    871\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:607\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    604\u001b[0m layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    605\u001b[0m past_key_value \u001b[38;5;241m=\u001b[39m past_key_values[i] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 607\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    608\u001b[0m     hidden_states,\n\u001b[1;32m    609\u001b[0m     attention_mask,\n\u001b[1;32m    610\u001b[0m     layer_head_mask,\n\u001b[1;32m    611\u001b[0m     encoder_hidden_states,  \u001b[38;5;66;03m# as a positional argument for gradient checkpointing\u001b[39;00m\n\u001b[1;32m    612\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m    613\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    614\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    615\u001b[0m )\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:508\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    498\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    507\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[1;32m    509\u001b[0m         hidden_states,\n\u001b[1;32m    510\u001b[0m         attention_mask,\n\u001b[1;32m    511\u001b[0m         head_mask,\n\u001b[1;32m    512\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    513\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[1;32m    514\u001b[0m     )\n\u001b[1;32m    515\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:435\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    427\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    433\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    434\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 435\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    436\u001b[0m         hidden_states,\n\u001b[1;32m    437\u001b[0m         attention_mask,\n\u001b[1;32m    438\u001b[0m         head_mask,\n\u001b[1;32m    439\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    440\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    441\u001b[0m         past_key_value,\n\u001b[1;32m    442\u001b[0m         output_attentions,\n\u001b[1;32m    443\u001b[0m     )\n\u001b[1;32m    444\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    445\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:358\u001b[0m, in \u001b[0;36mRobertaSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[1;32m    354\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    356\u001b[0m )\n\u001b[0;32m--> 358\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[1;32m    359\u001b[0m     query_layer,\n\u001b[1;32m    360\u001b[0m     key_layer,\n\u001b[1;32m    361\u001b[0m     value_layer,\n\u001b[1;32m    362\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    363\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_prob \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    364\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m    365\u001b[0m )\n\u001b[1;32m    367\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    368\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['roberta_prediction'] = df['llm_prediction'].progress_apply(lambda x: parse_result(roberta_pipe([x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1b01e167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163158d9c0f644f198b95411d15fb278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL_0\n",
      "LABEL_0\n",
      "LABEL_0\n",
      "LABEL_0\n",
      "LABEL_0\n",
      "LABEL_0\n",
      "LABEL_0\n",
      "LABEL_0\n",
      "LABEL_0\n",
      "LABEL_0\n",
      "LABEL_0\n",
      "LABEL_0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert_predction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllm_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: parse_result(distilbert_pipe([x])))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tqdm/std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(df, df_function)(wrapper, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4919\u001b[0m         func,\n\u001b[1;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1509\u001b[0m )\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tqdm/std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert_predction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllm_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: parse_result(distilbert_pipe([x])))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:163\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,)\n\u001b[0;32m--> 163\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:1445\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1442\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1443\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1444\u001b[0m     )\n\u001b[0;32m-> 1445\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(final_iterator)\n\u001b[1;32m   1446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:1371\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1370\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1371\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1372\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:194\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    193\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:917\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    915\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 917\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistilbert(\n\u001b[1;32m    918\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    919\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    920\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    921\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    922\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    923\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    924\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    925\u001b[0m )\n\u001b[1;32m    926\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    927\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:736\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[1;32m    732\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_attention_mask_for_sdpa(\n\u001b[1;32m    733\u001b[0m             attention_mask, embeddings\u001b[38;5;241m.\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    734\u001b[0m         )\n\u001b[0;32m--> 736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[1;32m    737\u001b[0m     x\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[1;32m    738\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    739\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    740\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    741\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    742\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    743\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:541\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    539\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> 541\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    542\u001b[0m     hidden_state,\n\u001b[1;32m    543\u001b[0m     attn_mask,\n\u001b[1;32m    544\u001b[0m     head_mask[i],\n\u001b[1;32m    545\u001b[0m     output_attentions,\n\u001b[1;32m    546\u001b[0m )\n\u001b[1;32m    548\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:494\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    491\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa_layer_norm(sa_output \u001b[38;5;241m+\u001b[39m x)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# Feed Forward Network\u001b[39;00m\n\u001b[0;32m--> 494\u001b[0m ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(sa_output)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    495\u001b[0m ffn_output: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer_norm(ffn_output \u001b[38;5;241m+\u001b[39m sa_output)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    497\u001b[0m output \u001b[38;5;241m=\u001b[39m (ffn_output,)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:428\u001b[0m, in \u001b[0;36mFFN.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m apply_chunking_to_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, \u001b[38;5;28minput\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pytorch_utils.py:250\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forward_fn(\u001b[38;5;241m*\u001b[39minput_tensors)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:431\u001b[0m, in \u001b[0;36mFFN.ff_chunk\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mff_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 431\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin1(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    432\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n\u001b[1;32m    433\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin2(x)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['distilbert_predction'] = df['llm_prediction'].progress_apply(lambda x: parse_result(distilbert_pipe([x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ed84c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>hipFracture</th>\n",
       "      <th>spineFracture</th>\n",
       "      <th>otherAdultFracture</th>\n",
       "      <th>fractureAge</th>\n",
       "      <th>fracturedBones</th>\n",
       "      <th>priorDexaScan</th>\n",
       "      <th>lastDexaScanDetails</th>\n",
       "      <th>fragilityFractureAfter45</th>\n",
       "      <th>youngLowTraumaFracture</th>\n",
       "      <th>...</th>\n",
       "      <th>spineSurgery</th>\n",
       "      <th>hipSurgery</th>\n",
       "      <th>gastricSurgery</th>\n",
       "      <th>currentlyPregnant</th>\n",
       "      <th>llm_data</th>\n",
       "      <th>llm_prediction</th>\n",
       "      <th>input_weight</th>\n",
       "      <th>total_weight</th>\n",
       "      <th>risk_level</th>\n",
       "      <th>roberta_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>34.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>42.0</td>\n",
       "      <td>Hip (Proximal Femur), Spine (Vertebrae), Pelvis</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>34.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>34.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>77.0</td>\n",
       "      <td>Wrist (Distal Radius), Humerus (Upper Arm Bone)</td>\n",
       "      <td>No</td>\n",
       "      <td>2024 at General hospital</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>10.5</td>\n",
       "      <td>34.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Spine (Vertebrae), Humerus (Upper Arm Bone)</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>\\n    The patient has reported a history of hi...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>34.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender hipFracture spineFracture otherAdultFracture  fractureAge  \\\n",
       "0  Female          No            No                 No          NaN   \n",
       "1  Female          No            No                Yes         42.0   \n",
       "2  Female          No            No                 No          NaN   \n",
       "3  Female          No            No                 No         77.0   \n",
       "4  Female          No           Yes                 No         66.0   \n",
       "\n",
       "                                    fracturedBones priorDexaScan  \\\n",
       "0                                              NaN            No   \n",
       "1  Hip (Proximal Femur), Spine (Vertebrae), Pelvis            No   \n",
       "2                                              NaN            No   \n",
       "3  Wrist (Distal Radius), Humerus (Upper Arm Bone)            No   \n",
       "4      Spine (Vertebrae), Humerus (Upper Arm Bone)            No   \n",
       "\n",
       "         lastDexaScanDetails fragilityFractureAfter45 youngLowTraumaFracture  \\\n",
       "0                        NaN                       No                     No   \n",
       "1                        NaN                       No                    Yes   \n",
       "2                        NaN                       No                     No   \n",
       "3  2024 at General hospital                        No                    Yes   \n",
       "4                        NaN                      Yes                     No   \n",
       "\n",
       "   ... spineSurgery hipSurgery gastricSurgery currentlyPregnant  \\\n",
       "0  ...           No         No             No                No   \n",
       "1  ...           No         No             No                No   \n",
       "2  ...           No         No             No                No   \n",
       "3  ...          Yes         No            Yes                No   \n",
       "4  ...           No         No             No               Yes   \n",
       "\n",
       "                                            llm_data  \\\n",
       "0  \\n    The patient has reported a history of hi...   \n",
       "1  \\n    The patient has reported a history of hi...   \n",
       "2  \\n    The patient has reported a history of hi...   \n",
       "3  \\n    The patient has reported a history of hi...   \n",
       "4  \\n    The patient has reported a history of hi...   \n",
       "\n",
       "                                      llm_prediction input_weight  \\\n",
       "0  \\n    The patient has reported a history of hi...          4.0   \n",
       "1  \\n    The patient has reported a history of hi...          8.0   \n",
       "2  \\n    The patient has reported a history of hi...          5.0   \n",
       "3  \\n    The patient has reported a history of hi...         10.5   \n",
       "4  \\n    The patient has reported a history of hi...         12.0   \n",
       "\n",
       "  total_weight risk_level roberta_prediction  \n",
       "0         34.5          2                  0  \n",
       "1         34.5          1                  1  \n",
       "2         34.5          2                  1  \n",
       "3         34.5          2                  1  \n",
       "4         34.5          2                  1  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e78702b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low: (1135, 58)\n",
      "Medium: (207, 58)\n",
      "High: (3538, 58)\n"
     ]
    }
   ],
   "source": [
    "print('Low:', df[df['risk_level'] == 0].shape)\n",
    "print('Medium:', df[df['risk_level'] == 1].shape)\n",
    "print('High:', df[df['risk_level'] == 2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6321bdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low: (562, 58)\n",
      "Medium: (4318, 58)\n",
      "High: (0, 58)\n"
     ]
    }
   ],
   "source": [
    "print('Low:', df[df['roberta_prediction'] == 0].shape)\n",
    "print('Medium:', df[df['roberta_prediction'] == 1].shape)\n",
    "print('High:', df[df['roberta_prediction'] == 2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542bf21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f84256c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = [\"Low\", \"Medium\", \"High\"]\n",
    "\n",
    "cm = confusion_matrix(df['risk_level'], df['roberta_prediction'])\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\",\", xticklabels=labels, yticklabels=labels, cmap=\"Blues\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for RoBERTa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c6011a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.17      0.23      1135\n",
      "           1       0.05      1.00      0.09       207\n",
      "           2       0.00      0.00      0.00      3538\n",
      "\n",
      "    accuracy                           0.08      4880\n",
      "   macro avg       0.13      0.39      0.11      4880\n",
      "weighted avg       0.08      0.08      0.06      4880\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['risk_level'], df['roberta_prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Low\", \"Medium\", \"High\"]\n",
    "\n",
    "cm = confusion_matrix(df['risk_level'], df['roberta_prediction'])\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\",\", xticklabels=labels, yticklabels=labels, cmap=\"Blues\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for DistilBERT')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
